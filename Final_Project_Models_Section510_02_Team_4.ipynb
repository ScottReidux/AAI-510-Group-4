{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Notebook Sections Summary**"
      ],
      "metadata": {
        "id": "l4QhdFdl3OKg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem statement and justification for the proposed approach.**\n",
        "\n",
        "Investors face significant challenges in predicting stock market movements.\n",
        "The volatility of the stock market makes it difficult to make informed investment decisions.\n",
        "This unpredictability can lead to potential financial losses.\n",
        "Recognizing the stress and uncertainty that investors experience.\n",
        "Acknowledging the need for reliable tools to aid in decision-making.\n",
        "Our goal is to provide a solution that helps investors optimize their portfolio.\n",
        "\n"
      ],
      "metadata": {
        "id": "mZthruhD0LkR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data understanding (EDA) – a graphical and non-graphical representation of relationships between the response variable and predictor variables.**\n",
        "\n",
        "Please see the seperate EDA notebook in Github."
      ],
      "metadata": {
        "id": "Eg-PX85D1MZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preperation**\n",
        "\n",
        "Please see the seperate EDA notebook in Github.\n",
        "\n",
        "Key Insights:\n",
        "\n",
        "Historical trends and patterns in stock prices\n",
        "\n",
        "Summary statistics like average, highs, and lows\n",
        "\n",
        "Visual Trend Analysis\n",
        "\n",
        "Generated plots for each stock observing closing price over time.\n",
        "\n",
        "Steps Taken:\n",
        "\n",
        "Date Conversion: Made dates readable by converting them to a standard format.\n",
        "\n",
        "Sorting: Arranged data by date for accurate analysis.\n",
        "\n",
        "Handling Missing Data: Filled in missing information to ensure completeness.\n",
        "\n",
        "Removing Duplicates: Ensured no repeated data entries.\n",
        "\n",
        "Stationarity Check: Made the data stable for better predictions.\n"
      ],
      "metadata": {
        "id": "EM2Q2F6s0bYF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Engineering**\n",
        "\n",
        "Enhancing Data - To help capture trends and patterns, improving prediction accuracy\n",
        "\n",
        "\n",
        "Moving Averages: Calculated average prices over different time periods.\n",
        "\n",
        "Relative Strength Index (RSI): Measured stock momentum.\n",
        "\n",
        "Past Prices: Included previous day prices as indicators.\n"
      ],
      "metadata": {
        "id": "bzWokOka1rJs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Selection**\n",
        "\n",
        "Choosing Important Factors - They have the highest impact on predicting future prices\n",
        "\n",
        "Selection Method: Used statistical methods to pick the most relevant features.\n",
        "\n",
        "Key Features Identified: Opening price, highest price, lowest price, volume, moving averages, RSI, and past prices.\n"
      ],
      "metadata": {
        "id": "2_3VJB0v1lct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modeling**\n",
        "\n",
        "Selection, comparison, tuning, and analysis – consider ensembles.\n",
        "\n",
        "There are four seperate models below with data prep and feature engineering to enhance their performance.\n",
        "\n",
        "Models: Random Forrest, XGBoost, LSTM, ARIMA\n",
        "\n"
      ],
      "metadata": {
        "id": "PQVoS4Eb2KvS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation – performance measures, results, and conclusions.**\n",
        "\n",
        "The individual models have individual performance statisics"
      ],
      "metadata": {
        "id": "gmNbqj6G22o6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Discussion and conclusions – address the problem statement and recommendation.**"
      ],
      "metadata": {
        "id": "NR_-7ng13JU4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**S&P 500 comparison with other high performing stocks**"
      ],
      "metadata": {
        "id": "I4xx78p7mF0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "\n",
        "# Define the list of stock tickers\n",
        "tickers = ['AAPL', 'GOOGL', 'GOOG', 'NVDA', 'AMZN', 'DASH', 'MSFT', 'UBER', 'TSLA', 'AMD', 'HPQ', 'DELL', 'META', 'SNOW', 'HOOD', 'PTON', 'AFRM', 'ROKU', 'WISH', 'TOST']\n",
        "index_ticker = '^GSPC'  # S&P 500\n",
        "\n",
        "# Define the date range\n",
        "start_date = '2014-01-01'\n",
        "end_date = '2024-01-01'\n",
        "\n",
        "# Function to calculate percentage change\n",
        "def calculate_percentage_change(data):\n",
        "    return ((data.iloc[-1] - data.iloc[0]) / data.iloc[0]) * 100\n",
        "\n",
        "# Download the historical data for the stocks\n",
        "data = yf.download(tickers, start=start_date, end=end_date)['Adj Close']\n",
        "\n",
        "# Download the historical data for the S&P 500 index\n",
        "sp500_data = yf.download(index_ticker, start=start_date, end=end_date)['Adj Close']\n",
        "\n",
        "# Calculate the percentage change over the entire period for each stock\n",
        "percentage_change = data.apply(calculate_percentage_change)\n",
        "\n",
        "# Calculate the percentage change over the entire period for the S&P 500 index\n",
        "sp500_change = calculate_percentage_change(sp500_data)\n",
        "\n",
        "# Compare each stock's performance to the S&P 500\n",
        "better_than_sp500 = percentage_change[percentage_change > sp500_change]\n",
        "\n",
        "# Display the results\n",
        "print(\"Stocks that performed better than the S&P 500 from 2014 to 2024:\")\n",
        "print(better_than_sp500)\n",
        "print(f\"\\nS&P 500 Percentage Change: {sp500_change:.2f}%\")"
      ],
      "metadata": {
        "id": "8xqwB0hWmKbB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dec7f21b-d133-4e74-9fb3-5df3e664558b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[**********************85%%***************       ]  17 of 20 completedWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: query2.finance.yahoo.com. Connection pool size: 10\n",
            "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: query2.finance.yahoo.com. Connection pool size: 10\n",
            "[*********************100%%**********************]  20 of 20 completed\n",
            "[*********************100%%**********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stocks that performed better than the S&P 500 from 2014 to 2024:\n",
            "Ticker\n",
            "AAPL       874.606343\n",
            "AMD       3631.898782\n",
            "AMZN       663.575139\n",
            "GOOG       408.330584\n",
            "GOOGL      401.474457\n",
            "META       546.974954\n",
            "MSFT       911.948359\n",
            "NVDA     12389.785733\n",
            "TSLA      2383.144411\n",
            "dtype: float64\n",
            "\n",
            "S&P 500 Percentage Change: 160.36%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stocks that performed better than the S&P 500 from 2014 to 2024:\n",
        "\n",
        "Ticker\n",
        "\n",
        "AAPL      1011.686759\n",
        "\n",
        "AMD       3631.898782\n",
        "\n",
        "AMZN       663.575139\n",
        "\n",
        "GOOG       408.330600\n",
        "\n",
        "GOOGL      401.474436\n",
        "\n",
        "HPQ        222.939383\n",
        "\n",
        "META       546.974967\n",
        "\n",
        "MSFT      1101.757924\n",
        "\n",
        "NVDA     13138.616066\n",
        "\n",
        "TSLA      2383.144411\n",
        "\n",
        "dtype: float64\n",
        "\n",
        "S&P 500 Percentage Change: 160.36%"
      ],
      "metadata": {
        "id": "vLb3b1WmmR_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stock performance using processed dataset**"
      ],
      "metadata": {
        "id": "qJaVy13EnG-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  The stock's performance using processed data\n",
        "#  Plot actual and predicted values for each stock\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "# Define the list of stock tickers and corresponding file paths\n",
        "tickers = ['NVDA', 'MSFT', 'AMD', 'AAPL', 'TSLA']\n",
        "file_paths = {\n",
        "    'NVDA': r'C:\\USD\\Machine Learning - Fundamentals AAI 510\\Final Team Project\\processed\\processed\\processed_nvda.us.txt',\n",
        "    'MSFT': r'C:\\USD\\Machine Learning - Fundamentals AAI 510\\Final Team Project\\processed\\processed\\processed_msft.us.txt',\n",
        "    'AMD': r'C:\\USD\\Machine Learning - Fundamentals AAI 510\\Final Team Project\\processed\\processed\\processed_amd.us.txt',\n",
        "    'AAPL': r'C:\\USD\\Machine Learning - Fundamentals AAI 510\\Final Team Project\\processed\\processed\\processed_aapl.us.txt',\n",
        "    'AMZN': r'C:\\USD\\Machine Learning - Fundamentals AAI 510\\Final Team Project\\processed\\processed\\processed_amzn.us.txt',\n",
        "    'TSLA': r'C:\\USD\\Machine Learning - Fundamentals AAI 510\\Final Team Project\\processed\\processed\\processed_tsla.us.txt'\n",
        "}\n",
        "\n",
        "# Define the date range\n",
        "start_date = '2014-01-01'\n",
        "end_date = datetime.today().strftime('%Y-%m-%d')\n",
        "\n",
        "# Function to create lag features\n",
        "def create_lag_features(df, lags):\n",
        "    for lag in range(1, lags + 1):\n",
        "        df[f'lag_{lag}'] = df['Close'].shift(lag)\n",
        "    df['rolling_mean_5'] = df['Close'].rolling(window=5).mean()\n",
        "    df['rolling_std_5'] = df['Close'].rolling(window=5).std()\n",
        "    df['rolling_mean_10'] = df['Close'].rolling(window=10).mean()\n",
        "    df['rolling_std_10'] = df['Close'].rolling(window=10).std()\n",
        "    df['rolling_mean_20'] = df['Close'].rolling(window=20).mean()\n",
        "    df['rolling_std_20'] = df['Close'].rolling(window=20).std()\n",
        "    df['momentum'] = df['Close'] - df['Close'].shift(4)\n",
        "    return df.dropna()\n",
        "\n",
        "# Function to normalize data\n",
        "def normalize_data(df):\n",
        "    return (df - df.min()) / (df.max() - df.min())\n",
        "\n",
        "# Function to fit and predict using Random Forest and XGBoost\n",
        "def fit_and_predict(df, ticker):\n",
        "    # Create lag features\n",
        "    df = create_lag_features(df, 7)\n",
        "\n",
        "    # Define features and target\n",
        "    features = [f'lag_{i}' for i in range(1, 8)] + ['rolling_mean_5', 'rolling_std_5', 'rolling_mean_10', 'rolling_std_10', 'rolling_mean_20', 'rolling_std_20', 'momentum']\n",
        "    X = df[features]\n",
        "    y = df['Close']\n",
        "\n",
        "    # Scale the features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Train test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "    # Hyperparameter tuning for Random Forest\n",
        "    rf_params = {\n",
        "        'n_estimators': [100, 200],\n",
        "        'max_depth': [10, 20, None],\n",
        "        'min_samples_split': [2, 5],\n",
        "        'min_samples_leaf': [1, 2],\n",
        "        'bootstrap': [True, False]\n",
        "    }\n",
        "    rf_model = RandomForestRegressor(random_state=42)\n",
        "    rf_random = RandomizedSearchCV(rf_model, rf_params, n_iter=20, cv=3, n_jobs=-1, random_state=42, scoring='neg_mean_squared_error')\n",
        "    rf_random.fit(X_train, y_train)\n",
        "    rf_best_model = rf_random.best_estimator_\n",
        "\n",
        "    # Predict with Random Forest\n",
        "    y_pred_rf = rf_best_model.predict(X_test)\n",
        "\n",
        "    # Hyperparameter tuning for XGBoost\n",
        "    xgb_params = {\n",
        "        'n_estimators': [100, 200],\n",
        "        'max_depth': [3, 6],\n",
        "        'learning_rate': [0.01, 0.1],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0]\n",
        "    }\n",
        "    xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "    xgb_random = RandomizedSearchCV(xgb_model, xgb_params, n_iter=20, cv=3, n_jobs=-1, random_state=42, scoring='neg_mean_squared_error')\n",
        "    xgb_random.fit(X_train, y_train)\n",
        "    xgb_best_model = xgb_random.best_estimator_\n",
        "\n",
        "    # Predict with XGBoost\n",
        "    y_pred_xgb = xgb_best_model.predict(X_test)\n",
        "\n",
        "    # Normalize the data for comparison\n",
        "    df['Close'] = normalize_data(df['Close'])\n",
        "    y_test = normalize_data(y_test)\n",
        "    y_pred_rf = normalize_data(pd.Series(y_pred_rf, index=y_test.index))\n",
        "    y_pred_xgb = normalize_data(pd.Series(y_pred_xgb, index=y_test.index))\n",
        "\n",
        "    # Plot actual vs predicted\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(df.index[-len(y_test):], y_test, label='Actual', marker='o')\n",
        "    plt.plot(df.index[-len(y_test):], y_pred_rf, label='Random Forest Predicted', linestyle='--')\n",
        "    plt.plot(df.index[-len(y_test):], y_pred_xgb, label='XGBoost Predicted', linestyle='--')\n",
        "    plt.title(f'{ticker} Stock Prediction')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Normalized Close Price')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Print evaluation metrics\n",
        "    print(f'{ticker} Random Forest MSE: {mean_squared_error(y_test, y_pred_rf):.4f}')\n",
        "    print(f'{ticker} XGBoost MSE: {mean_squared_error(y_test, y_pred_xgb):.4f}')\n",
        "    print(f'{ticker} Random Forest MAE: {mean_absolute_error(y_test, y_pred_rf):.4f}')\n",
        "    print(f'{ticker} XGBoost MAE: {mean_absolute_error(y_test, y_pred_xgb):.4f}')\n",
        "    print(f'{ticker} Random Forest R²: {r2_score(y_test, y_pred_rf):.4f}')\n",
        "    print(f'{ticker} XGBoost R²: {r2_score(y_test, y_pred_xgb):.4f}')\n",
        "\n",
        "# Load and process each stock data\n",
        "for ticker in tickers:\n",
        "    print(f'\\nProcessing {ticker}...\\n')\n",
        "    file_path = file_paths[ticker]\n",
        "    df_ticker = pd.read_csv(file_path)\n",
        "    df_ticker['Date'] = pd.to_datetime(df_ticker['Date'])\n",
        "    df_ticker.set_index('Date', inplace=True)\n",
        "    fit_and_predict(df_ticker, ticker)"
      ],
      "metadata": {
        "id": "Z6RDh60MnEvu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "collapsed": true,
        "outputId": "99269d5d-1024-406f-afa3-ab9524062139"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing NVDA...\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'C:\\\\USD\\\\Machine Learning - Fundamentals AAI 510\\\\Final Team Project\\\\processed\\\\processed\\\\processed_nvda.us.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-93173f42294c>\u001b[0m in \u001b[0;36m<cell line: 122>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\nProcessing {ticker}...\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mdf_ticker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mdf_ticker\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_ticker\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mdf_ticker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\USD\\\\Machine Learning - Fundamentals AAI 510\\\\Final Team Project\\\\processed\\\\processed\\\\processed_nvda.us.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stock performance using historical data again SP 500**"
      ],
      "metadata": {
        "id": "nsTyL3asmuS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Compare each stock's performance against S&P500 using historical data.\n",
        "#  Plot actual and predicted values for each stock along withe S&P500 index values\n",
        "\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the list of stock tickers\n",
        "tickers = ['NVDA', 'MSFT', 'AMD', 'AAPL', 'AMZN']\n",
        "index_ticker = '^GSPC'  # S&P 500\n",
        "\n",
        "# Define the date range\n",
        "start_date = '2014-01-01'\n",
        "end_date = '2024-01-01'\n",
        "\n",
        "# Function to create lag features\n",
        "def create_lag_features(df, lags):\n",
        "    for lag in range(1, lags + 1):\n",
        "        df[f'lag_{lag}'] = df['Close'].shift(lag)\n",
        "    df['rolling_mean_5'] = df['Close'].rolling(window=5).mean()\n",
        "    df['rolling_std_5'] = df['Close'].rolling(window=5).std()\n",
        "    return df.dropna()\n",
        "\n",
        "# Function to normalize data\n",
        "def normalize_data(df):\n",
        "    return (df - df.min()) / (df.max() - df.min())\n",
        "\n",
        "# Function to fit and predict using Random Forest and XGBoost\n",
        "def fit_and_predict(df, sp500_df, ticker):\n",
        "    # Create lag features\n",
        "    df = create_lag_features(df, 7)\n",
        "\n",
        "    # Define features and target\n",
        "    features = [f'lag_{i}' for i in range(1, 8)] + ['rolling_mean_5', 'rolling_std_5']\n",
        "    X = df[features]\n",
        "    y = df['Close']\n",
        "\n",
        "    # Scale the features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Train test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "    # Hyperparameter tuning for Random Forest\n",
        "    rf_params = {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [10, 20, None],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    }\n",
        "    rf_model = RandomForestRegressor(random_state=42)\n",
        "    rf_grid = GridSearchCV(rf_model, rf_params, cv=3, n_jobs=-1, scoring='neg_mean_squared_error')\n",
        "    rf_grid.fit(X_train, y_train)\n",
        "    rf_best_model = rf_grid.best_estimator_\n",
        "\n",
        "    # Predict with Random Forest\n",
        "    y_pred_rf = rf_best_model.predict(X_test)\n",
        "\n",
        "    # Hyperparameter tuning for XGBoost\n",
        "    xgb_params = {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [3, 6, 9],\n",
        "        'learning_rate': [0.01, 0.1, 0.2]\n",
        "    }\n",
        "    xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "    xgb_grid = GridSearchCV(xgb_model, xgb_params, cv=3, n_jobs=-1, scoring='neg_mean_squared_error')\n",
        "    xgb_grid.fit(X_train, y_train)\n",
        "    xgb_best_model = xgb_grid.best_estimator_\n",
        "\n",
        "    # Predict with XGBoost\n",
        "    y_pred_xgb = xgb_best_model.predict(X_test)\n",
        "\n",
        "    # Normalize the data for comparison\n",
        "    df['Close'] = normalize_data(df['Close'])\n",
        "    y_test = normalize_data(y_test)\n",
        "    y_pred_rf = normalize_data(pd.Series(y_pred_rf, index=y_test.index))\n",
        "    y_pred_xgb = normalize_data(pd.Series(y_pred_xgb, index=y_test.index))\n",
        "    sp500_df['Close'] = normalize_data(sp500_df['Close'])\n",
        "\n",
        "    # Plot actual vs predicted with S&P 500 index\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(df.index[-len(y_test):], y_test, label='Actual', marker='o')\n",
        "    plt.plot(df.index[-len(y_test):], y_pred_rf, label='Random Forest Predicted', linestyle='--')\n",
        "    plt.plot(df.index[-len(y_test):], y_pred_xgb, label='XGBoost Predicted', linestyle='--')\n",
        "    plt.plot(sp500_df.index[-len(y_test):], sp500_df['Close'][-len(y_test):], label='S&P 500 Index', linestyle=':')\n",
        "    plt.title(f'{ticker} Stock Prediction vs S&P 500')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Normalized Close Price')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Print evaluation metrics\n",
        "    print(f'{ticker} Random Forest MSE: {mean_squared_error(y_test, y_pred_rf):.4f}')\n",
        "    print(f'{ticker} XGBoost MSE: {mean_squared_error(y_test, y_pred_xgb):.4f}')\n",
        "    print(f'{ticker} Random Forest MAE: {mean_absolute_error(y_test, y_pred_rf):.4f}')\n",
        "    print(f'{ticker} XGBoost MAE: {mean_absolute_error(y_test, y_pred_xgb):.4f}')\n",
        "    print(f'{ticker} Random Forest R²: {r2_score(y_test, y_pred_rf):.4f}')\n",
        "    print(f'{ticker} XGBoost R²: {r2_score(y_test, y_pred_xgb):.4f}')\n",
        "\n",
        "# Download historical data for the stocks and S&P 500\n",
        "data = yf.download(tickers, start=start_date, end=end_date)\n",
        "sp500_data = yf.download(index_ticker, start=start_date, end=end_date)['Adj Close'].reset_index()\n",
        "sp500_data = sp500_data.rename(columns={'Adj Close': 'Close'}).set_index('Date')\n",
        "\n",
        "# Fit and predict for each stock\n",
        "for ticker in tickers:\n",
        "    print(f'\\nProcessing {ticker}...\\n')\n",
        "    df_ticker = data['Adj Close'][ticker].reset_index().rename(columns={ticker: 'Close'})\n",
        "    df_ticker.set_index('Date', inplace=True)\n",
        "    fit_and_predict(df_ticker, sp500_data, ticker)"
      ],
      "metadata": {
        "id": "rTXXWRYDmoE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM Model**"
      ],
      "metadata": {
        "id": "NjcZajFFjvIJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Target Stocks**\n",
        "\n",
        "Apple - AAPL\n",
        "\n",
        "Google (Alphabet Inc.) - GOOGL (for Class A shares)\n",
        "\n",
        "Nvidia - NVDA\n",
        "\n",
        "Amazon - AMZN\n",
        "\n",
        "Microsoft - MSFT\n",
        "\n",
        "AMD (Advanced Micro Devices) - AMD\n",
        "\n",
        "HP (HP Inc., not to be confused with Hewlett Packard Enterprise) - HPQ\n",
        "\n",
        "QUALCOMM Incorporated - QCOM\n",
        "\n",
        "Salesforce, Inc. - CRM\n",
        "\n",
        "Cisco Systems, Inc. - CSCO\n",
        "\n",
        "Oracle Corporation - ORCL"
      ],
      "metadata": {
        "id": "tQTupmrllcsR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCbMq0eJia1l"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv('/content/combined_stocks.csv')\n",
        "\n",
        "# Print the first 5 rows of the DataFrame\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import yfinance as yf\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv('/content/combined_stocks.csv')\n",
        "\n",
        "# Convert the 'Date' column to datetime type\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "# Filtering data for the relevant years\n",
        "df = df[(df['Date'] < '2019-01-01')]\n",
        "\n",
        "# Download S&P 500 data\n",
        "index_data = yf.download('^GSPC', start='2016-11-01', end='2017-11-01')\n",
        "index_data.reset_index(inplace=True)  # Resetting the index so 'Date' becomes a column\n",
        "index_data = index_data[['Date', 'Close']]\n",
        "index_data['Percent_Return'] = index_data['Close'].pct_change()\n",
        "\n",
        "# Resample to weekly data and sum percent returns\n",
        "index_data.set_index('Date', inplace=True)  # Set 'Date' as the index again for resampling\n",
        "index_data = index_data.resample('W').sum()  # Summing weekly percent returns\n",
        "\n",
        "# List of tickers to process\n",
        "tickers = ['NVDA', 'MSFT', 'AMD', 'AAPL', 'AMZN']\n",
        "\n",
        "results = {}\n",
        "\n",
        "for ticker in tickers:\n",
        "    print(f\"Processing {ticker}\")\n",
        "    df_ticker = df[df['Ticker'] == ticker]\n",
        "    df_ticker.set_index('Date', inplace=True)\n",
        "    df_ticker.loc[:, 'Percent_Return'] = df_ticker['Close'].pct_change()\n",
        "    df_ticker = df_ticker.resample('W').sum()  # Summing weekly percent returns\n",
        "\n",
        "    # Prepare data for LSTM\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    returns = df_ticker['Percent_Return'].dropna().values.reshape(-1, 1)\n",
        "    scaled_data = scaler.fit_transform(returns)\n",
        "\n",
        "    # Create the training data\n",
        "    n_past = 4  # Adjusted for weekly data\n",
        "    X_train, y_train = [], []\n",
        "    for i in range(n_past, len(scaled_data)-52):  # Reserving the last year for testing, adjusted for weeks\n",
        "        X_train.append(scaled_data[i-n_past:i, 0])\n",
        "        y_train.append(scaled_data[i, 0])\n",
        "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "\n",
        "    # Build the LSTM model\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(units=50, return_sequences=False))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    # Compile and fit the model\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    model.fit(X_train, y_train, epochs=20, batch_size=32)\n",
        "\n",
        "    # Predicting values\n",
        "    test_data = scaled_data[-(n_past+52):]  # Last year data + 4 weeks\n",
        "    X_test = []\n",
        "    for i in range(n_past, len(test_data)):\n",
        "        X_test.append(test_data[i-n_past:i, 0])\n",
        "    X_test = np.array(X_test)\n",
        "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
        "    predicted_returns = model.predict(X_test)\n",
        "    predicted_returns = scaler.inverse_transform(predicted_returns)\n",
        "\n",
        "    # Actual percent returns\n",
        "    actual_returns = df_ticker['Percent_Return'].tail(52).values  # Last year of weekly data\n",
        "\n",
        "    # Plotting the results\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(df_ticker.index[-52:], actual_returns, color='blue', label='Actual Percent Returns')\n",
        "    plt.plot(df_ticker.index[-52:], predicted_returns[:, 0], color='red', label='Predicted Percent Returns')\n",
        "\n",
        "    # Include S&P 500 percent returns for comparison\n",
        "    sp500_returns = index_data['Percent_Return'].tail(52).dropna()  # last year weekly data\n",
        "    plt.plot(sp500_returns.index, sp500_returns.values, color='green', label='S&P 500 Percent Returns')\n",
        "\n",
        "    plt.title(f'Weekly Percent Return Prediction for {ticker} vs S&P 500 (Ending 2018-04-01)')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Percent Return')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Store results\n",
        "    results[ticker] = predicted_returns\n",
        "\n",
        "    # Calculate statistics\n",
        "    mse = mean_squared_error(actual_returns, predicted_returns[:, 0])\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(actual_returns, predicted_returns[:, 0])\n",
        "    mape = np.mean(np.abs((actual_returns - predicted_returns[:, 0]) / actual_returns)) * 100\n",
        "    r2 = r2_score(actual_returns, predicted_returns[:, 0])\n",
        "\n",
        "    # Print statistics\n",
        "    print(f\"Metrics for {ticker}: MSE: {mse}, RMSE: {rmse}, MAE: {mae}, MAPE: {mape}%, R^2: {r2}\")"
      ],
      "metadata": {
        "id": "T3Vqw9UYj1ZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ARIMA Model** (not used in presentation)"
      ],
      "metadata": {
        "id": "lHd_FErGkzFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv('/content/combined_stocks.csv')\n",
        "\n",
        "# Print the first 5 rows of the DataFrame\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "y3Tyy707k5XJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert 'Date' to datetime if not already done\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "# Define start and end dates\n",
        "start_date = '2008-01-01'\n",
        "end_date = '2018-01-01'\n",
        "\n",
        "# Slice the DataFrame to include only data within the specified date range\n",
        "# Make a copy to avoid SettingWithCopyWarning when modifying this slice\n",
        "selected_data = df[(df['Date'] >= start_date) & (df['Date'] <= end_date)].copy()\n",
        "\n",
        "# Calculate daily returns\n",
        "selected_data['Daily_Return'] = selected_data.groupby('Ticker')['Close'].pct_change()\n",
        "\n",
        "# Pivot the DataFrame to make each ticker's returns a column\n",
        "pivot_df = selected_data.pivot(index='Date', columns='Ticker', values='Daily_Return')\n",
        "\n",
        "# Print the first 5 rows of the DataFrame\n",
        "print(selected_data.head())\n"
      ],
      "metadata": {
        "id": "yR6A6VvhlBPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting\n",
        "plt.figure(figsize=(14, 7))\n",
        "for column in pivot_df.columns:\n",
        "    plt.plot(pivot_df.index, pivot_df[column], label=column)\n",
        "\n",
        "plt.title('Daily Returns of Different Stocks')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Daily Return')\n",
        "plt.legend(title='Ticker')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P1viYbxylEcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "\n",
        "# Convert 'Date' to datetime if it's your DataFrame index or a column\n",
        "selected_data['Date'] = pd.to_datetime(selected_data['Date'])\n",
        "df.set_index('Date', inplace=True)\n",
        "\n",
        "# List of unique tickers\n",
        "tickers = selected_data['Ticker'].unique()\n",
        "\n",
        "# Loop through each ticker and plot ACF and PACF\n",
        "for ticker in tickers:\n",
        "    data = selected_data[selected_data['Ticker'] == ticker]['Close']\n",
        "\n",
        "    # Plot ACF\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plot_acf(data, lags=50, title=f'Autocorrelation Function for {ticker}')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot PACF\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plot_pacf(data, lags=50, title=f'Partial Autocorrelation Function for {ticker}')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "lhbYUY_VlICd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "\n",
        "# Convert 'Date' to datetime type and set it as the DataFrame index\n",
        "selected_data['Date'] = pd.to_datetime(selected_data['Date'])\n",
        "selected_data.set_index('Date', inplace=True)\n",
        "selected_data.index = pd.DatetimeIndex(selected_data.index).to_period('D')  # Assuming daily data\n",
        "\n",
        "# List of tickers\n",
        "tickers = ['aapl', 'googl', 'nvda', 'amzn', 'msft', 'amd', 'hpq', 'qcom', 'crm', 'csco', 'orcl']\n",
        "\n",
        "# Iterate through each ticker\n",
        "for ticker in tickers:\n",
        "    selected_data_ticker = selected_data[selected_data['Ticker'].str.lower() == ticker.lower()]\n",
        "    closing_prices = selected_data_ticker['Close']\n",
        "\n",
        "    # Fit an ARIMA model (potentially adjust p, d, q based on data characteristics)\n",
        "    model = ARIMA(closing_prices, order=(1, 1, 1))\n",
        "    model_fit = model.fit()\n",
        "\n",
        "    # Forecasting\n",
        "    forecast = model_fit.get_forecast(steps=365*2)  # 2 years\n",
        "    mean_forecast = forecast.predicted_mean\n",
        "    confidence_intervals = forecast.conf_int()\n",
        "\n",
        "    # Convert PeriodIndex to DateTimeIndex for plotting\n",
        "    closing_prices.index = closing_prices.index.to_timestamp()\n",
        "    mean_forecast.index = mean_forecast.index.to_timestamp()\n",
        "\n",
        "    # Plotting the results\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(closing_prices.index, closing_prices, label='Actual')\n",
        "    plt.plot(mean_forecast.index, mean_forecast, color='red', label='Forecast')\n",
        "    plt.fill_between(mean_forecast.index,\n",
        "                     confidence_intervals['lower Close'],\n",
        "                     confidence_intervals['upper Close'], color='pink')\n",
        "    plt.title(f'{ticker.upper()} Stock Price Forecast')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Price')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "nh_74Ap7lQF8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}