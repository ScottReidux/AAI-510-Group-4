{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**S&P 500 comparison with other high performing stocks**"
      ],
      "metadata": {
        "id": "I4xx78p7mF0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "\n",
        "# Define the list of stock tickers\n",
        "tickers = ['AAPL', 'GOOGL', 'GOOG', 'NVDA', 'AMZN', 'DASH', 'MSFT', 'UBER', 'TSLA', 'AMD', 'HPQ', 'DELL', 'META', 'SNOW', 'HOOD', 'PTON', 'AFRM', 'ROKU', 'WISH', 'TOST']\n",
        "index_ticker = '^GSPC'  # S&P 500\n",
        "\n",
        "# Define the date range\n",
        "start_date = '2014-01-01'\n",
        "end_date = '2024-01-01'\n",
        "\n",
        "# Function to calculate percentage change\n",
        "def calculate_percentage_change(data):\n",
        "    return ((data.iloc[-1] - data.iloc[0]) / data.iloc[0]) * 100\n",
        "\n",
        "# Download the historical data for the stocks\n",
        "data = yf.download(tickers, start=start_date, end=end_date)['Adj Close']\n",
        "\n",
        "# Download the historical data for the S&P 500 index\n",
        "sp500_data = yf.download(index_ticker, start=start_date, end=end_date)['Adj Close']\n",
        "\n",
        "# Calculate the percentage change over the entire period for each stock\n",
        "percentage_change = data.apply(calculate_percentage_change)\n",
        "\n",
        "# Calculate the percentage change over the entire period for the S&P 500 index\n",
        "sp500_change = calculate_percentage_change(sp500_data)\n",
        "\n",
        "# Compare each stock's performance to the S&P 500\n",
        "better_than_sp500 = percentage_change[percentage_change > sp500_change]\n",
        "\n",
        "# Display the results\n",
        "print(\"Stocks that performed better than the S&P 500 from 2014 to 2024:\")\n",
        "print(better_than_sp500)\n",
        "print(f\"\\nS&P 500 Percentage Change: {sp500_change:.2f}%\")"
      ],
      "metadata": {
        "id": "8xqwB0hWmKbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stocks that performed better than the S&P 500 from 2014 to 2024:\n",
        "\n",
        "Ticker\n",
        "\n",
        "AAPL      1011.686759\n",
        "\n",
        "AMD       3631.898782\n",
        "\n",
        "AMZN       663.575139\n",
        "\n",
        "GOOG       408.330600\n",
        "\n",
        "GOOGL      401.474436\n",
        "\n",
        "HPQ        222.939383\n",
        "\n",
        "META       546.974967\n",
        "\n",
        "MSFT      1101.757924\n",
        "\n",
        "NVDA     13138.616066\n",
        "\n",
        "TSLA      2383.144411\n",
        "\n",
        "dtype: float64\n",
        "\n",
        "S&P 500 Percentage Change: 160.36%"
      ],
      "metadata": {
        "id": "vLb3b1WmmR_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stock performance using processed dataset**"
      ],
      "metadata": {
        "id": "qJaVy13EnG-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  The stock's performance using processed data\n",
        "#  Plot actual and predicted values for each stock\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "# Define the list of stock tickers and corresponding file paths\n",
        "tickers = ['NVDA', 'MSFT', 'AMD', 'AAPL', 'TSLA']\n",
        "file_paths = {\n",
        "    'NVDA': r'C:\\USD\\Machine Learning - Fundamentals AAI 510\\Final Team Project\\processed\\processed\\processed_nvda.us.txt',\n",
        "    'MSFT': r'C:\\USD\\Machine Learning - Fundamentals AAI 510\\Final Team Project\\processed\\processed\\processed_msft.us.txt',\n",
        "    'AMD': r'C:\\USD\\Machine Learning - Fundamentals AAI 510\\Final Team Project\\processed\\processed\\processed_amd.us.txt',\n",
        "    'AAPL': r'C:\\USD\\Machine Learning - Fundamentals AAI 510\\Final Team Project\\processed\\processed\\processed_aapl.us.txt',\n",
        "    'AMZN': r'C:\\USD\\Machine Learning - Fundamentals AAI 510\\Final Team Project\\processed\\processed\\processed_amzn.us.txt',\n",
        "    'TSLA': r'C:\\USD\\Machine Learning - Fundamentals AAI 510\\Final Team Project\\processed\\processed\\processed_tsla.us.txt'\n",
        "}\n",
        "\n",
        "# Define the date range\n",
        "start_date = '2014-01-01'\n",
        "end_date = datetime.today().strftime('%Y-%m-%d')\n",
        "\n",
        "# Function to create lag features\n",
        "def create_lag_features(df, lags):\n",
        "    for lag in range(1, lags + 1):\n",
        "        df[f'lag_{lag}'] = df['Close'].shift(lag)\n",
        "    df['rolling_mean_5'] = df['Close'].rolling(window=5).mean()\n",
        "    df['rolling_std_5'] = df['Close'].rolling(window=5).std()\n",
        "    df['rolling_mean_10'] = df['Close'].rolling(window=10).mean()\n",
        "    df['rolling_std_10'] = df['Close'].rolling(window=10).std()\n",
        "    df['rolling_mean_20'] = df['Close'].rolling(window=20).mean()\n",
        "    df['rolling_std_20'] = df['Close'].rolling(window=20).std()\n",
        "    df['momentum'] = df['Close'] - df['Close'].shift(4)\n",
        "    return df.dropna()\n",
        "\n",
        "# Function to normalize data\n",
        "def normalize_data(df):\n",
        "    return (df - df.min()) / (df.max() - df.min())\n",
        "\n",
        "# Function to fit and predict using Random Forest and XGBoost\n",
        "def fit_and_predict(df, ticker):\n",
        "    # Create lag features\n",
        "    df = create_lag_features(df, 7)\n",
        "\n",
        "    # Define features and target\n",
        "    features = [f'lag_{i}' for i in range(1, 8)] + ['rolling_mean_5', 'rolling_std_5', 'rolling_mean_10', 'rolling_std_10', 'rolling_mean_20', 'rolling_std_20', 'momentum']\n",
        "    X = df[features]\n",
        "    y = df['Close']\n",
        "\n",
        "    # Scale the features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Train test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "    # Hyperparameter tuning for Random Forest\n",
        "    rf_params = {\n",
        "        'n_estimators': [100, 200],\n",
        "        'max_depth': [10, 20, None],\n",
        "        'min_samples_split': [2, 5],\n",
        "        'min_samples_leaf': [1, 2],\n",
        "        'bootstrap': [True, False]\n",
        "    }\n",
        "    rf_model = RandomForestRegressor(random_state=42)\n",
        "    rf_random = RandomizedSearchCV(rf_model, rf_params, n_iter=20, cv=3, n_jobs=-1, random_state=42, scoring='neg_mean_squared_error')\n",
        "    rf_random.fit(X_train, y_train)\n",
        "    rf_best_model = rf_random.best_estimator_\n",
        "\n",
        "    # Predict with Random Forest\n",
        "    y_pred_rf = rf_best_model.predict(X_test)\n",
        "\n",
        "    # Hyperparameter tuning for XGBoost\n",
        "    xgb_params = {\n",
        "        'n_estimators': [100, 200],\n",
        "        'max_depth': [3, 6],\n",
        "        'learning_rate': [0.01, 0.1],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0]\n",
        "    }\n",
        "    xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "    xgb_random = RandomizedSearchCV(xgb_model, xgb_params, n_iter=20, cv=3, n_jobs=-1, random_state=42, scoring='neg_mean_squared_error')\n",
        "    xgb_random.fit(X_train, y_train)\n",
        "    xgb_best_model = xgb_random.best_estimator_\n",
        "\n",
        "    # Predict with XGBoost\n",
        "    y_pred_xgb = xgb_best_model.predict(X_test)\n",
        "\n",
        "    # Normalize the data for comparison\n",
        "    df['Close'] = normalize_data(df['Close'])\n",
        "    y_test = normalize_data(y_test)\n",
        "    y_pred_rf = normalize_data(pd.Series(y_pred_rf, index=y_test.index))\n",
        "    y_pred_xgb = normalize_data(pd.Series(y_pred_xgb, index=y_test.index))\n",
        "\n",
        "    # Plot actual vs predicted\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(df.index[-len(y_test):], y_test, label='Actual', marker='o')\n",
        "    plt.plot(df.index[-len(y_test):], y_pred_rf, label='Random Forest Predicted', linestyle='--')\n",
        "    plt.plot(df.index[-len(y_test):], y_pred_xgb, label='XGBoost Predicted', linestyle='--')\n",
        "    plt.title(f'{ticker} Stock Prediction')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Normalized Close Price')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Print evaluation metrics\n",
        "    print(f'{ticker} Random Forest MSE: {mean_squared_error(y_test, y_pred_rf):.4f}')\n",
        "    print(f'{ticker} XGBoost MSE: {mean_squared_error(y_test, y_pred_xgb):.4f}')\n",
        "    print(f'{ticker} Random Forest MAE: {mean_absolute_error(y_test, y_pred_rf):.4f}')\n",
        "    print(f'{ticker} XGBoost MAE: {mean_absolute_error(y_test, y_pred_xgb):.4f}')\n",
        "    print(f'{ticker} Random Forest R²: {r2_score(y_test, y_pred_rf):.4f}')\n",
        "    print(f'{ticker} XGBoost R²: {r2_score(y_test, y_pred_xgb):.4f}')\n",
        "\n",
        "# Load and process each stock data\n",
        "for ticker in tickers:\n",
        "    print(f'\\nProcessing {ticker}...\\n')\n",
        "    file_path = file_paths[ticker]\n",
        "    df_ticker = pd.read_csv(file_path)\n",
        "    df_ticker['Date'] = pd.to_datetime(df_ticker['Date'])\n",
        "    df_ticker.set_index('Date', inplace=True)\n",
        "    fit_and_predict(df_ticker, ticker)"
      ],
      "metadata": {
        "id": "Z6RDh60MnEvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stock performance using historical data again SP 500**"
      ],
      "metadata": {
        "id": "nsTyL3asmuS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Compare each stock's performance against S&P500 using historical data.\n",
        "#  Plot actual and predicted values for each stock along withe S&P500 index values\n",
        "\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the list of stock tickers\n",
        "tickers = ['NVDA', 'MSFT', 'AMD', 'AAPL', 'AMZN']\n",
        "index_ticker = '^GSPC'  # S&P 500\n",
        "\n",
        "# Define the date range\n",
        "start_date = '2014-01-01'\n",
        "end_date = '2024-01-01'\n",
        "\n",
        "# Function to create lag features\n",
        "def create_lag_features(df, lags):\n",
        "    for lag in range(1, lags + 1):\n",
        "        df[f'lag_{lag}'] = df['Close'].shift(lag)\n",
        "    df['rolling_mean_5'] = df['Close'].rolling(window=5).mean()\n",
        "    df['rolling_std_5'] = df['Close'].rolling(window=5).std()\n",
        "    return df.dropna()\n",
        "\n",
        "# Function to normalize data\n",
        "def normalize_data(df):\n",
        "    return (df - df.min()) / (df.max() - df.min())\n",
        "\n",
        "# Function to fit and predict using Random Forest and XGBoost\n",
        "def fit_and_predict(df, sp500_df, ticker):\n",
        "    # Create lag features\n",
        "    df = create_lag_features(df, 7)\n",
        "\n",
        "    # Define features and target\n",
        "    features = [f'lag_{i}' for i in range(1, 8)] + ['rolling_mean_5', 'rolling_std_5']\n",
        "    X = df[features]\n",
        "    y = df['Close']\n",
        "\n",
        "    # Scale the features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Train test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "    # Hyperparameter tuning for Random Forest\n",
        "    rf_params = {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [10, 20, None],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    }\n",
        "    rf_model = RandomForestRegressor(random_state=42)\n",
        "    rf_grid = GridSearchCV(rf_model, rf_params, cv=3, n_jobs=-1, scoring='neg_mean_squared_error')\n",
        "    rf_grid.fit(X_train, y_train)\n",
        "    rf_best_model = rf_grid.best_estimator_\n",
        "\n",
        "    # Predict with Random Forest\n",
        "    y_pred_rf = rf_best_model.predict(X_test)\n",
        "\n",
        "    # Hyperparameter tuning for XGBoost\n",
        "    xgb_params = {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [3, 6, 9],\n",
        "        'learning_rate': [0.01, 0.1, 0.2]\n",
        "    }\n",
        "    xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "    xgb_grid = GridSearchCV(xgb_model, xgb_params, cv=3, n_jobs=-1, scoring='neg_mean_squared_error')\n",
        "    xgb_grid.fit(X_train, y_train)\n",
        "    xgb_best_model = xgb_grid.best_estimator_\n",
        "\n",
        "    # Predict with XGBoost\n",
        "    y_pred_xgb = xgb_best_model.predict(X_test)\n",
        "\n",
        "    # Normalize the data for comparison\n",
        "    df['Close'] = normalize_data(df['Close'])\n",
        "    y_test = normalize_data(y_test)\n",
        "    y_pred_rf = normalize_data(pd.Series(y_pred_rf, index=y_test.index))\n",
        "    y_pred_xgb = normalize_data(pd.Series(y_pred_xgb, index=y_test.index))\n",
        "    sp500_df['Close'] = normalize_data(sp500_df['Close'])\n",
        "\n",
        "    # Plot actual vs predicted with S&P 500 index\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(df.index[-len(y_test):], y_test, label='Actual', marker='o')\n",
        "    plt.plot(df.index[-len(y_test):], y_pred_rf, label='Random Forest Predicted', linestyle='--')\n",
        "    plt.plot(df.index[-len(y_test):], y_pred_xgb, label='XGBoost Predicted', linestyle='--')\n",
        "    plt.plot(sp500_df.index[-len(y_test):], sp500_df['Close'][-len(y_test):], label='S&P 500 Index', linestyle=':')\n",
        "    plt.title(f'{ticker} Stock Prediction vs S&P 500')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Normalized Close Price')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Print evaluation metrics\n",
        "    print(f'{ticker} Random Forest MSE: {mean_squared_error(y_test, y_pred_rf):.4f}')\n",
        "    print(f'{ticker} XGBoost MSE: {mean_squared_error(y_test, y_pred_xgb):.4f}')\n",
        "    print(f'{ticker} Random Forest MAE: {mean_absolute_error(y_test, y_pred_rf):.4f}')\n",
        "    print(f'{ticker} XGBoost MAE: {mean_absolute_error(y_test, y_pred_xgb):.4f}')\n",
        "    print(f'{ticker} Random Forest R²: {r2_score(y_test, y_pred_rf):.4f}')\n",
        "    print(f'{ticker} XGBoost R²: {r2_score(y_test, y_pred_xgb):.4f}')\n",
        "\n",
        "# Download historical data for the stocks and S&P 500\n",
        "data = yf.download(tickers, start=start_date, end=end_date)\n",
        "sp500_data = yf.download(index_ticker, start=start_date, end=end_date)['Adj Close'].reset_index()\n",
        "sp500_data = sp500_data.rename(columns={'Adj Close': 'Close'}).set_index('Date')\n",
        "\n",
        "# Fit and predict for each stock\n",
        "for ticker in tickers:\n",
        "    print(f'\\nProcessing {ticker}...\\n')\n",
        "    df_ticker = data['Adj Close'][ticker].reset_index().rename(columns={ticker: 'Close'})\n",
        "    df_ticker.set_index('Date', inplace=True)\n",
        "    fit_and_predict(df_ticker, sp500_data, ticker)"
      ],
      "metadata": {
        "id": "rTXXWRYDmoE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM Model**"
      ],
      "metadata": {
        "id": "NjcZajFFjvIJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Target Stocks**\n",
        "\n",
        "Apple - AAPL\n",
        "\n",
        "Google (Alphabet Inc.) - GOOGL (for Class A shares)\n",
        "\n",
        "Nvidia - NVDA\n",
        "\n",
        "Amazon - AMZN\n",
        "\n",
        "Microsoft - MSFT\n",
        "\n",
        "AMD (Advanced Micro Devices) - AMD\n",
        "\n",
        "HP (HP Inc., not to be confused with Hewlett Packard Enterprise) - HPQ\n",
        "\n",
        "QUALCOMM Incorporated - QCOM\n",
        "\n",
        "Salesforce, Inc. - CRM\n",
        "\n",
        "Cisco Systems, Inc. - CSCO\n",
        "\n",
        "Oracle Corporation - ORCL"
      ],
      "metadata": {
        "id": "tQTupmrllcsR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCbMq0eJia1l"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv('/content/combined_stocks.csv')\n",
        "\n",
        "# Print the first 5 rows of the DataFrame\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import yfinance as yf\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv('/content/combined_stocks.csv')\n",
        "\n",
        "# Convert the 'Date' column to datetime type\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "# Filtering data for the relevant years\n",
        "df = df[(df['Date'] < '2019-01-01')]\n",
        "\n",
        "# Download S&P 500 data\n",
        "index_data = yf.download('^GSPC', start='2016-11-01', end='2017-11-01')\n",
        "index_data.reset_index(inplace=True)  # Resetting the index so 'Date' becomes a column\n",
        "index_data = index_data[['Date', 'Close']]\n",
        "index_data['Percent_Return'] = index_data['Close'].pct_change()\n",
        "\n",
        "# Resample to weekly data and sum percent returns\n",
        "index_data.set_index('Date', inplace=True)  # Set 'Date' as the index again for resampling\n",
        "index_data = index_data.resample('W').sum()  # Summing weekly percent returns\n",
        "\n",
        "# List of tickers to process\n",
        "tickers = ['NVDA', 'MSFT', 'AMD', 'AAPL', 'AMZN']\n",
        "\n",
        "results = {}\n",
        "\n",
        "for ticker in tickers:\n",
        "    print(f\"Processing {ticker}\")\n",
        "    df_ticker = df[df['Ticker'] == ticker]\n",
        "    df_ticker.set_index('Date', inplace=True)\n",
        "    df_ticker.loc[:, 'Percent_Return'] = df_ticker['Close'].pct_change()\n",
        "    df_ticker = df_ticker.resample('W').sum()  # Summing weekly percent returns\n",
        "\n",
        "    # Prepare data for LSTM\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    returns = df_ticker['Percent_Return'].dropna().values.reshape(-1, 1)\n",
        "    scaled_data = scaler.fit_transform(returns)\n",
        "\n",
        "    # Create the training data\n",
        "    n_past = 4  # Adjusted for weekly data\n",
        "    X_train, y_train = [], []\n",
        "    for i in range(n_past, len(scaled_data)-52):  # Reserving the last year for testing, adjusted for weeks\n",
        "        X_train.append(scaled_data[i-n_past:i, 0])\n",
        "        y_train.append(scaled_data[i, 0])\n",
        "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "\n",
        "    # Build the LSTM model\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(units=50, return_sequences=False))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    # Compile and fit the model\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    model.fit(X_train, y_train, epochs=20, batch_size=32)\n",
        "\n",
        "    # Predicting values\n",
        "    test_data = scaled_data[-(n_past+52):]  # Last year data + 4 weeks\n",
        "    X_test = []\n",
        "    for i in range(n_past, len(test_data)):\n",
        "        X_test.append(test_data[i-n_past:i, 0])\n",
        "    X_test = np.array(X_test)\n",
        "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
        "    predicted_returns = model.predict(X_test)\n",
        "    predicted_returns = scaler.inverse_transform(predicted_returns)\n",
        "\n",
        "    # Actual percent returns\n",
        "    actual_returns = df_ticker['Percent_Return'].tail(52).values  # Last year of weekly data\n",
        "\n",
        "    # Plotting the results\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(df_ticker.index[-52:], actual_returns, color='blue', label='Actual Percent Returns')\n",
        "    plt.plot(df_ticker.index[-52:], predicted_returns[:, 0], color='red', label='Predicted Percent Returns')\n",
        "\n",
        "    # Include S&P 500 percent returns for comparison\n",
        "    sp500_returns = index_data['Percent_Return'].tail(52).dropna()  # last year weekly data\n",
        "    plt.plot(sp500_returns.index, sp500_returns.values, color='green', label='S&P 500 Percent Returns')\n",
        "\n",
        "    plt.title(f'Weekly Percent Return Prediction for {ticker} vs S&P 500 (Ending 2018-04-01)')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Percent Return')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Store results\n",
        "    results[ticker] = predicted_returns\n",
        "\n",
        "    # Calculate statistics\n",
        "    mse = mean_squared_error(actual_returns, predicted_returns[:, 0])\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(actual_returns, predicted_returns[:, 0])\n",
        "    mape = np.mean(np.abs((actual_returns - predicted_returns[:, 0]) / actual_returns)) * 100\n",
        "    r2 = r2_score(actual_returns, predicted_returns[:, 0])\n",
        "\n",
        "    # Print statistics\n",
        "    print(f\"Metrics for {ticker}: MSE: {mse}, RMSE: {rmse}, MAE: {mae}, MAPE: {mape}%, R^2: {r2}\")"
      ],
      "metadata": {
        "id": "T3Vqw9UYj1ZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ARIMA Model** (not used in presentation)"
      ],
      "metadata": {
        "id": "lHd_FErGkzFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv('/content/combined_stocks.csv')\n",
        "\n",
        "# Print the first 5 rows of the DataFrame\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "y3Tyy707k5XJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert 'Date' to datetime if not already done\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "# Define start and end dates\n",
        "start_date = '2008-01-01'\n",
        "end_date = '2018-01-01'\n",
        "\n",
        "# Slice the DataFrame to include only data within the specified date range\n",
        "# Make a copy to avoid SettingWithCopyWarning when modifying this slice\n",
        "selected_data = df[(df['Date'] >= start_date) & (df['Date'] <= end_date)].copy()\n",
        "\n",
        "# Calculate daily returns\n",
        "selected_data['Daily_Return'] = selected_data.groupby('Ticker')['Close'].pct_change()\n",
        "\n",
        "# Pivot the DataFrame to make each ticker's returns a column\n",
        "pivot_df = selected_data.pivot(index='Date', columns='Ticker', values='Daily_Return')\n",
        "\n",
        "# Print the first 5 rows of the DataFrame\n",
        "print(selected_data.head())\n"
      ],
      "metadata": {
        "id": "yR6A6VvhlBPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting\n",
        "plt.figure(figsize=(14, 7))\n",
        "for column in pivot_df.columns:\n",
        "    plt.plot(pivot_df.index, pivot_df[column], label=column)\n",
        "\n",
        "plt.title('Daily Returns of Different Stocks')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Daily Return')\n",
        "plt.legend(title='Ticker')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P1viYbxylEcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "\n",
        "# Convert 'Date' to datetime if it's your DataFrame index or a column\n",
        "selected_data['Date'] = pd.to_datetime(selected_data['Date'])\n",
        "df.set_index('Date', inplace=True)\n",
        "\n",
        "# List of unique tickers\n",
        "tickers = selected_data['Ticker'].unique()\n",
        "\n",
        "# Loop through each ticker and plot ACF and PACF\n",
        "for ticker in tickers:\n",
        "    data = selected_data[selected_data['Ticker'] == ticker]['Close']\n",
        "\n",
        "    # Plot ACF\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plot_acf(data, lags=50, title=f'Autocorrelation Function for {ticker}')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot PACF\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plot_pacf(data, lags=50, title=f'Partial Autocorrelation Function for {ticker}')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "lhbYUY_VlICd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "\n",
        "# Convert 'Date' to datetime type and set it as the DataFrame index\n",
        "selected_data['Date'] = pd.to_datetime(selected_data['Date'])\n",
        "selected_data.set_index('Date', inplace=True)\n",
        "selected_data.index = pd.DatetimeIndex(selected_data.index).to_period('D')  # Assuming daily data\n",
        "\n",
        "# List of tickers\n",
        "tickers = ['aapl', 'googl', 'nvda', 'amzn', 'msft', 'amd', 'hpq', 'qcom', 'crm', 'csco', 'orcl']\n",
        "\n",
        "# Iterate through each ticker\n",
        "for ticker in tickers:\n",
        "    selected_data_ticker = selected_data[selected_data['Ticker'].str.lower() == ticker.lower()]\n",
        "    closing_prices = selected_data_ticker['Close']\n",
        "\n",
        "    # Fit an ARIMA model (potentially adjust p, d, q based on data characteristics)\n",
        "    model = ARIMA(closing_prices, order=(1, 1, 1))\n",
        "    model_fit = model.fit()\n",
        "\n",
        "    # Forecasting\n",
        "    forecast = model_fit.get_forecast(steps=365*2)  # 2 years\n",
        "    mean_forecast = forecast.predicted_mean\n",
        "    confidence_intervals = forecast.conf_int()\n",
        "\n",
        "    # Convert PeriodIndex to DateTimeIndex for plotting\n",
        "    closing_prices.index = closing_prices.index.to_timestamp()\n",
        "    mean_forecast.index = mean_forecast.index.to_timestamp()\n",
        "\n",
        "    # Plotting the results\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(closing_prices.index, closing_prices, label='Actual')\n",
        "    plt.plot(mean_forecast.index, mean_forecast, color='red', label='Forecast')\n",
        "    plt.fill_between(mean_forecast.index,\n",
        "                     confidence_intervals['lower Close'],\n",
        "                     confidence_intervals['upper Close'], color='pink')\n",
        "    plt.title(f'{ticker.upper()} Stock Price Forecast')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Price')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "nh_74Ap7lQF8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}